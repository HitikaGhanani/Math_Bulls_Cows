# Math_Bulls_Cows
https://www.youtube.com/watch?v=heaEKqW0GqQ
https://www.youtube.com/watch?v=VuoPjr4qR7M

This project uses Shannon entropy to measure how much uncertainty remains about the secret number at every turn of the game.

1. What entropy represents
At any point in the game, we maintain a set:
ğ‘† = all possible secret numbers still consistent with the feedback

Entropy is: H(S)=log2âˆ£Sâˆ£

This value tells us how many bits of uncertainty we have about the secret.
âˆ£Sâˆ£ is large â†’ high entropy â†’ we know very little
âˆ£Sâˆ£ is small â†’ low entropy â†’ we are close to the answer
âˆ£Sâˆ£=1 â†’ entropy = 0 â†’ the secret is logically determined

Entropy is the core mathematical measurement of our knowledge.
â€‹
2. How entropy changes after each guess

When the player makes a guess, we calculate Bulls and Cows
Then we filter the candidate set: ğ‘† ={sâˆˆS âˆ£bulls_cows(s,guess)=(b,c)}

New entropy: H(Sâ€²)=log2âˆ£Sâ€²âˆ£
This always drops, meaning uncertainty decreases.
Entropy is printed after every guess so the player can watch their uncertainty shrink in real time.

3. How entropy helps build the best strategy
We can predict how informative each potential guess might be by calculating Expected Information Gain.â€‹
The best guess is the one with the highest expected information gain.

4. When entropy becomes zero

Entropy becomes zero when: âˆ£Sâˆ£=1
This means we logically know the secret.

Entropy staying at zero is correct but uncertainty cannot go negative.
